TorchInductor with X86 CPU as backend of Quantization in PyTorch 2.0 Export
=============================================================================

**Author**: `Leslie Fang <https://github.com/leslie-fang-intel>`_, `Weiwen Xia <https://github.com/Xia-Weiwen>`_, `Jiong Gong <https://github.com/jgong5>`_, `Jerry Zhang <https://github.com/jerryzh168>`_

Prerequisites
^^^^^^^^^^^^^^^

-  `PyTorch 2.0 Export Post Training Static Quantization <https://pytorch.org/tutorials/prototype/pt2e_quant_ptq_static.html>`_
-  `TorchInductor and torch.compile concepts in PyTorch <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_

Introduction
^^^^^^^^^^^^^^

This tutorial introduces the steps to use PyTorch 2.0 Export Quantization flow and TorchInductor to do post-training static quantization
in graph mode on X86 CPU devices.

- New Quantization 2.0 flow uses the PT2 Export workflow to capture the model into a graph and perform quantization transformations on top of the ATen dialect graph. This approach is expected to have significantly higher model coverage, better programmability, and a simplified UX.
- TorchInductor is the new compiler backend that compiles the FX Graphs generated by TorchDynamo into optimized C++/Triton kernels.

The high-level architecture of quantization 2.0 with Inductor could look like this:

::

    float_model(Python)                               Input
        \                                              /
         \                                            /
    —--------------------------------------------------------
    |                    Torch Export                       |
    —--------------------------------------------------------
                                |
                        FX Graph in ATen     
                                |            X86InductorQuantizer
                                |                 /
                                |                /
    —--------------------------------------------------------
    |                    prepare_pt2e                       |
    —--------------------------------------------------------
                                |
                         Calibrate/Train
                                |
    —--------------------------------------------------------
    |                      convert_pt2e                     |
    —--------------------------------------------------------
                                |
                    Reference Quantized Model
                                |
    —--------------------------------------------------------
    |                        Lowering                       |
    —--------------------------------------------------------
                                |
                            Inductor

Combining Quantization in PyTorch 2.0 Export and TorchInductor, we have flexibility and productivity with the new Quantization frontend
and outstanding out-of-box performance with the compiler backend. Especially on Intel fourth generation (SPR) Xeon processors which can
further boost the models' performance by leveraging the
`advanced-matrix-extensions <https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html>`_ feature.

Now, we will have a step-by-step tutorial for how to use it with `torchvision resnet18 model <https://download.pytorch.org/models/resnet18-f37072fd.pth>`_.

1. Capture FX Graph
---------------------

We will start by doing the necessary imports, capturing the FX Graph from the eager module.

::

    import torch
    import torchvision.models as models
    import copy
    from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
    import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
    from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer
    from torch._export import capture_pre_autograd_graph

    # Create the Eager Model
    model_name = "resnet18"
    model = models.__dict__[model_name](pretrained=True)

    # Set the model to eval mode
    model = model.eval()

    # Create the data, using the dummy data here as an example
    traced_bs = 50
    x = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)
    example_inputs = (x,)

    # Capture the FX Graph to be quantized
    with torch.no_grad():
        # For PyTorch Master, using the API of `capture_pre_autograd_graph`
        # Note 1: `capture_pre_autograd_graph` is also a short-term API, it will be updated to use the official `torch.export` API when that is ready.
        exported_model = capture_pre_autograd_graph(
            model,
            example_inputs
        )
        # Note 2: For PyTorch 2.1 release, please use the API of `torch._dynamo.export` to capture the FX Graph
        # exported_model, guards = torch._dynamo.export(
        #     model,
        #     *copy.deepcopy(example_inputs),
        #     aten_graph=True,
        # )


Now, we will have the FX Module to be quantized.

2. Apply Quantization Flow
----------------------------

After we capture the FX Module to be quantized, we will import the Backend Quantizer for X86 CPU and configure how to
quantize the model.

::

    # Create X86InductorQuantizer
    quantizer = X86InductorQuantizer()
    quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())

.. note::

- The default quantization configuration in ``X86InductorQuantizer`` uses 8-bits for both weights and activations, as indicated by the argument ``reduce_range=False``.
  When Vector Neural Network Instruction is not available, the onednn backend silently chooses kernels that assume
  `multiplications are 7-bit x 8-bit <https://oneapi-src.github.io/oneDNN/dev_guide_int8_computations.html#inputs-of-mixed-type-u8-and-s8>`_. In other words, potential
  numeric saturation and accuracy issue may happen when running on CPU without Vector Neural Network Instruction.

After we import the backend-specific Quantizer, we will prepare the model for post-training static quantization.
``prepare_pt2e`` folds BatchNorm operators into preceding Conv2d operators, and inserts observers in appropriate places in the model.

::

    # PT2E Quantization flow
    prepared_model = prepare_pt2e(exported_model, quantizer)

Now, we will calibrate the ``prepared_model`` after the observers are inserted in the model.

::

    # We use the dummy data as an example here
    prepared_model(*example_inputs)

    # Alternatively: user can define the dataset to calibrate
    # def calibrate(model, data_loader):
    #     model.eval()
    #     with torch.no_grad():
    #         for image, target in data_loader:
    #             model(image)
    # calibrate(prepared_model, data_loader_test)  # run calibration on sample data

Finally, we will convert the calibrated Model to a quantized Model. ``convert_pt2e`` takes a calibrated model and produces a reference quantized model.

::

    converted_model = convert_pt2e(prepared_model).eval()

After these steps, we finished applying the quantization flow and we will get the reference quantized model.


3. Lower into Inductor
------------------------

After we get the reference quantized model, we will further lower it into the inductor backend.

::

    optimized_model = torch.compile(converted_model)

    # Running some benchmark
    optimized_model(*example_inputs)


Put all these codes together, we will have the `toy example code <https://gist.github.com/leslie-fang-intel/3f1652ef8296ce916717e938d887e86a>`_.
Please note that since the Inductor ``freeze`` feature does not default turn on yet, run your example code with ``TORCHINDUCTOR_FREEZING=1``.
For example:

::

    TORCHINDUCTOR_FREEZING=1 python example_x86inductorquantizer_pytorch_2_1.py

4. Conclusion
---------------

With this tutorial, we introduce how to use Inductor with X86 CPU in PyTorch 2.0 Quantization. Users can learn about
how to use ``X86InductorQuantizer`` to quantize a model and lower it into the inductor with X86 CPU devices.
